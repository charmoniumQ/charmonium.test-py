# A platform for testing computational experiments

## Introduction

_Define "computational experiments", why are they important, why bother reproducing them._
(Dan: I think this is really about experiments that are captured as workflows, not computational experiments more generally?)

_The result of this work is a software platform. The platform can be used to systematically gather, run, and store the results of computational experiments._

_What is the audience of this software?_

_Lay out the other sections._

## Prior work

| Language | Registry | First Author | DOI |
|------------|-----|------|---------|
| R | Dataverse | Trisovic | 10.1038/s41597-022-01143-6|
| Python/Jupyter Notebook | GitHub | Murta | 10.1109/MSR.2019.00077 |
| Python/Jupyter Notebook | GitHub | Kuo | 10.1145/3324884.3416585 |
| Python/Pytest | GitHub | Gruber | 10.1109/ICST49551.2021.00026 |
| Taverna | myExperiment | Zhao | 10.1109/eScience.2012.6404482 |
| Any | Any | Grayson | This work |

(Dan: There's a lot of work on computational reproducibility in general, most of which may or may not be relevant depending on what the defintion of "computational experiments" is, such as [Guix](https://guix.gnu.org), tools that are part of https://www.repronim.org, stuff from Tanu Malik like SciUnit (https://cdmdicewebprd01.dpu.depaul.edu/projects/advancedcontainers/), ... )
## Methodology

_Describe software system. Describe the rationale. Why did we use Dask, etc.?_

## Results

_current count of experiments, count of working experiments, storage cost per experiment, CPU cost per experiment_

_Efficiency := CPU time inside unerlying expeirment / total CPU time._

_Security analysis_

## Discussion

_How easy is it to do a query on the results?_

_Why should people use this?_

_What prior work exists_

## Conclusion

_Is this useful? We think so. What kinds of future research does this enable?_

## Appendix I: Instructions for reproducing

_Source code archive_

[1]: https://www.acm.org/publications/badging-terms
